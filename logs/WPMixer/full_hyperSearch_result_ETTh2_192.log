Args in experiment: Namespace(model='WPMixer', task_name='long_term_forecast', data='ETTh2', use_hyperParam_optim=False, no_decomposition=False, use_multi_gpu=False, n_jobs=1, seed=42, seq_len=96, pred_len=192, d_model=256, tfactor=3, dfactor=8, wavelet='db2', level=3, patch_len=16, stride=8, batch_size=256, learning_rate=0.000294929, dropout=0.0, embedding_dropout=0.0, weight_decay=0.0, patience=12, train_epochs=30, label_len=0, seasonal_patterns='Monthly', features='M', target='OT', freq='h', checkpoints='./checkpoints/', cols=None, num_workers=0, itr=1, lradj='type3', use_amp=True, use_gpu=True, gpu=0, devices='0,1', embed=0, loss='smoothL1', pct_start=0.2, optuna_seq_len=None, optuna_lr=None, optuna_batch=None, optuna_wavelet=None, optuna_tfactor=None, optuna_dfactor=None, optuna_epochs=None, optuna_dropout=None, optuna_embedding_dropout=None, optuna_patch_len=None, optuna_stride=None, optuna_lradj=None, optuna_dmodel=None, optuna_weight_decay=None, optuna_patience=None, optuna_level=None, optuna_trial_num=None, data_path='ETTh2.csv', root_path='./data/ETT/', c_in=7, c_out=7, detail_freq='h')
Use GPU: cuda:0
Start Training- WPMixer_ETTh2_dec-True_sl96_pl192_dm256_bt256_wvdb2_tf3_df8_ptl16_stl8_sd42
train 8353
val 2689
test 2689
Epoch 1: cost time: 1.31 sec
	Epoch 1: Steps- 32 | Train Loss: 0.18462 Vali.MSE: 0.28697 Vali.MAE: 0.36296 Test.MSE: 0.36226 Test.MAE: 0.38853
	Validation loss decreased (inf --> 0.286965).  Saving model ...
Updating learning rate to 0.000294929
Epoch 2: cost time: 1.03 sec
	Epoch 2: Steps- 32 | Train Loss: 0.17400 Vali.MSE: 0.27826 Vali.MAE: 0.35224 Test.MSE: 0.35131 Test.MAE: 0.37761
	Validation loss decreased (0.286965 --> 0.278259).  Saving model ...
Updating learning rate to 0.000294929
Epoch 3: cost time: 1.02 sec
	Epoch 3: Steps- 32 | Train Loss: 0.17020 Vali.MSE: 0.28230 Vali.MAE: 0.35530 Test.MSE: 0.35419 Test.MAE: 0.37883
	EarlyStopping counter: 1 out of 12
Updating learning rate to 0.000294929
Epoch 4: cost time: 1.03 sec
	Epoch 4: Steps- 32 | Train Loss: 0.16692 Vali.MSE: 0.28223 Vali.MAE: 0.35698 Test.MSE: 0.35365 Test.MAE: 0.38007
	EarlyStopping counter: 2 out of 12
Updating learning rate to 0.0002654361
Epoch 5: cost time: 1.02 sec
	Epoch 5: Steps- 32 | Train Loss: 0.16315 Vali.MSE: 0.28209 Vali.MAE: 0.35605 Test.MSE: 0.35311 Test.MAE: 0.37877
	EarlyStopping counter: 3 out of 12
Updating learning rate to 0.00023889249
Epoch 6: cost time: 1.02 sec
	Epoch 6: Steps- 32 | Train Loss: 0.16036 Vali.MSE: 0.28450 Vali.MAE: 0.35889 Test.MSE: 0.35233 Test.MAE: 0.38088
	EarlyStopping counter: 4 out of 12
Updating learning rate to 0.000215003241
Epoch 7: cost time: 1.00 sec
	Epoch 7: Steps- 32 | Train Loss: 0.15700 Vali.MSE: 0.28756 Vali.MAE: 0.35985 Test.MSE: 0.35783 Test.MAE: 0.38236
	EarlyStopping counter: 5 out of 12
Updating learning rate to 0.00019350291689999998
Epoch 8: cost time: 1.01 sec
	Epoch 8: Steps- 32 | Train Loss: 0.15528 Vali.MSE: 0.29246 Vali.MAE: 0.36207 Test.MSE: 0.35226 Test.MAE: 0.38129
	EarlyStopping counter: 6 out of 12
Updating learning rate to 0.00017415262521000002
Epoch 9: cost time: 1.01 sec
	Epoch 9: Steps- 32 | Train Loss: 0.15301 Vali.MSE: 0.29035 Vali.MAE: 0.36284 Test.MSE: 0.35345 Test.MAE: 0.38385
	EarlyStopping counter: 7 out of 12
Updating learning rate to 0.000156737362689
Epoch 10: cost time: 1.00 sec
	Epoch 10: Steps- 32 | Train Loss: 0.14988 Vali.MSE: 0.29294 Vali.MAE: 0.36333 Test.MSE: 0.35750 Test.MAE: 0.38499
	EarlyStopping counter: 8 out of 12
Updating learning rate to 0.0001410636264201
Epoch 11: cost time: 1.01 sec
	Epoch 11: Steps- 32 | Train Loss: 0.14835 Vali.MSE: 0.29656 Vali.MAE: 0.36487 Test.MSE: 0.35955 Test.MAE: 0.38579
	EarlyStopping counter: 9 out of 12
Updating learning rate to 0.00012695726377809003
Epoch 12: cost time: 1.03 sec
	Epoch 12: Steps- 32 | Train Loss: 0.14613 Vali.MSE: 0.29414 Vali.MAE: 0.36477 Test.MSE: 0.36216 Test.MAE: 0.38776
	EarlyStopping counter: 10 out of 12
Updating learning rate to 0.00011426153740028102
Epoch 13: cost time: 1.02 sec
	Epoch 13: Steps- 32 | Train Loss: 0.14412 Vali.MSE: 0.29925 Vali.MAE: 0.36680 Test.MSE: 0.36343 Test.MAE: 0.38768
	EarlyStopping counter: 11 out of 12
Updating learning rate to 0.00010283538366025293
Epoch 14: cost time: 1.00 sec
	Epoch 14: Steps- 32 | Train Loss: 0.14183 Vali.MSE: 0.29841 Vali.MAE: 0.36625 Test.MSE: 0.36583 Test.MAE: 0.38945
	EarlyStopping counter: 12 out of 12
	Early stopping
Start Testing- WPMixer_ETTh2_dec-True_sl96_pl192_dm256_bt256_wvdb2_tf3_df8_ptl16_stl8_sd42
test 2689
mse: 0.35131025314331055, mae: 0.37761080265045166
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.
Total GFLOPs: 84.2808
Total GPARAMs: 0.0086
